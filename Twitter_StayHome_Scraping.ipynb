{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "#scraping\n",
    "import GetOldTweets3 as got\n",
    "import time\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Twitter data through web-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(text_query, start_date, end_date, lang, location, within):\n",
    "   \n",
    "    # specifying tweet search criteria \n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(text_query)\\\n",
    "                          .setSince(start_date)\\\n",
    "                          .setUntil(end_date)\\\n",
    "                          .setLang(lang)\\\n",
    "                          .setNear(location)\\\n",
    "                          .setWithin(within)\n",
    "    \n",
    "    # scraping tweets based on criteria\n",
    "    tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    \n",
    "    # creating list of tweets with the tweet attributes \n",
    "    # specified in the list comprehension\n",
    "    text_tweets = [[tw.username,\n",
    "                tw.text,\n",
    "                tw.date,\n",
    "                tw.retweets,\n",
    "                tw.favorites,\n",
    "                tw.hashtags] for tw in tweet]\n",
    "    \n",
    "    # creating dataframe, assigning column names to list of\n",
    "    # tweets corresponding to tweet attributes\n",
    "    tw_df = pd.DataFrame(text_tweets, \n",
    "                         columns = ['User', 'Text', 'Date', 'Retweets', 'Favorites', 'HashTags'])\n",
    "    \n",
    "    return tw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected 10 cities which have large population and the number of confirmed cases of their county is large.  \n",
    "This is based on data from JHU on 2020-04-13.\n",
    "  \n",
    "Selected cities:  \n",
    "New York City, New York  (New York county 103208 1st)  \n",
    "Boston, Massachusetts  (Suffolk county 20934 20th)    \n",
    "Chicago, Illinois  (Cook county 14585 5th)  \n",
    "Detroit, Michigan  (Wayne county 11164 6th)  \n",
    "Los Angeles, California  (Los Angeles county 8894 8th)  \n",
    "Houston, Texas  (Harris county 3747 26th)  \n",
    "Newark, New Jersey  (Essex county 7410 11th)  \n",
    "Miami, Florida  (Miami-dade county 7058 12th)  \n",
    "Philadelphia, Pennsylvania  (Philadelphia county 6386 13th)  \n",
    "New Orleans, Louisiana\t(Orleans county 5600 17th)  \n",
    "  \n",
    "Reference for population: http://www.citymayors.com/gratis/uscities_100.html    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set attributes needed for get_tweets\n",
    "text_query = '#StayHome'\n",
    "start_date = '2020-03-05'\n",
    "end_date = '2020-04-12'\n",
    "lang = 'en'\n",
    "within = '50mi'\n",
    "\n",
    "citys = ['New York City, New York','Boston, Massachusetts','Chicago, Illinois','Detroit, Michigan','Los Angeles, California','Houston, Texas','Newark, New Jersey','Miami, Florida','Philadelphia, Pennsylvania','New Orleans, Louisiana']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap needed info and export to csv file\n",
    "for city in citys:\n",
    "    location = city\n",
    "    df = get_tweets(text_query, start_date, end_date, lang, location, within)\n",
    "    df.to_csv(city + '.csv', index = False)\n",
    "    time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'City' column and combine all datasets\n",
    "city_column = ['NY', 'BOSTON', 'CHI', 'DETROIT', 'LA', 'HOUSTON', 'NEWARK', 'MIA', 'PHIL', 'NEW ORLEANS']\n",
    "stayhome = pd.DataFrame()\n",
    "for i in range(len(city_column)):\n",
    "    df = pd.read_csv(citys[i] + '.csv')\n",
    "    df['City'] = city_column[i]\n",
    "    stayhome = pd.concat([stayhome, df], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export our raw data\n",
    "stayhome.to_csv('StayHome.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_csv(\"StayHome.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df.replace('[]', np.nan,inplace=True)\n",
    "df.drop_duplicates(inplace =True)\n",
    "df.drop_duplicates(subset = ['Text'],inplace =True)\n",
    "\n",
    "# devide timestamp into date and time\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['date'] = df['Date'].apply( lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "df['time'] = df['Date'].apply( lambda x: x.strftime(\"%H-%M-%S\"))\n",
    "df.drop(['Date'],axis = 1, inplace =True)\n",
    "\n",
    "# replace city initials with city names\n",
    "replace_values = {'NY' : 'New York', 'BOSTON' : 'Boston', 'CHI' : 'Chicago', 'DETROIT' : 'Detroit', 'HOUSTON' : 'Houston',\n",
    "                  'LA' : 'Los Angeles', 'MIA' : 'Miami', 'NEWARK' : 'Newark', 'NEW ORLEANS' : 'New Orleans', 'PHIL' : 'Philadelphia'}                                                                                          \n",
    "df = df.replace({\"City\": replace_values})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate users per day\n",
    "user1 = pd.DataFrame(df.groupby('date')['User'].nunique())\n",
    "user2 = pd.DataFrame(df.groupby('date')['Text'].nunique())\n",
    "user = pd.concat([user1,user2],axis =1)\n",
    "user['num_of_text_per_capita'] = user['Text']/user['User']\n",
    "user['total_unique_user'] = unique_user\n",
    "user['lag_user'] = lag_user\n",
    "user['new_user'] = user['total_unique_user']-user['lag_user']\n",
    "user.drop(['lag_user'],axis = 1, inplace = True)\n",
    "user.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data is imbalanced in user number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of unique users for different time period\n",
    "dates = sorted(df['date'].unique())\n",
    "unique_user = list()\n",
    "for date in dates: \n",
    "    unique_user.append(df[df['date'] < date]['User'].nunique())\n",
    "\n",
    "lag_user = [0]+unique_user \n",
    "lag_user = lag_user[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select '2020-04-06' as division\n",
    "# old users: who started to be active before 04-06\n",
    "# new_users: who weren't active until 04-06\n",
    "existing_user= df[df['date'] < '2020-04-06']['User'].unique()\n",
    "\n",
    "# retain all old users\n",
    "df1 = df[df['User'].isin(existing_user)]                      \n",
    "df2 = df[-df['User'].isin(existing_user)]\n",
    "\n",
    "# sample new users\n",
    "df2 = df2.sample(1300)\n",
    "\n",
    "# get new sample\n",
    "df = pd.concat([df1,df2])                                     \n",
    "\n",
    "# export new sample\n",
    "df.to_csv(\"StayHome_final.csv\", index = False)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of our new sample\n",
    "fig = plt.figure(figsize = (12,5))\n",
    "chart = sns.countplot(df['date'], order = sorted(df['date'].unique()))\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate users per day\n",
    "user1 = pd.DataFrame(df.groupby('date')['User'].nunique())\n",
    "user2 = pd.DataFrame(df.groupby('date')['Text'].nunique())\n",
    "user = pd.concat([user1,user2],axis =1)\n",
    "user['num_of_text_per_capita'] = user['Text']/user['User']\n",
    "user['total_unique_user'] = unique_user\n",
    "user['lag_user'] = lag_user\n",
    "user['new_user'] = user['total_unique_user']-user['lag_user']\n",
    "user.drop(['lag_user'],axis =1, inplace = True)\n",
    "user.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New sample is balanced in user number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
